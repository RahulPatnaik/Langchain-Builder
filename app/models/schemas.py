from pydantic import BaseModel, Field
from typing import List, Optional, Dict, Any, Literal

# --- Chat Schemas ---

class ChatMessage(BaseModel):
    role: Literal["user", "assistant", "system"] = Field(..., description="Role of the message sender (user, assistant, system)")
    content: str = Field(..., description="Content of the message")

class ChatRequest(BaseModel):
    messages: List[ChatMessage] = Field(..., min_items=1, description="List of messages in the conversation history")
    # Add other optional parameters for LLM if needed (e.g., temperature, max_tokens)
    # temperature: Optional[float] = Field(None, ge=0.0, le=2.0)
    # model: Optional[str] = Field(None, description="Override default model")

class ChatResponse(BaseModel):
    role: Literal["assistant"] = "assistant"
    content: str = Field(..., description="The LLM's response message")

class StreamingChatResponse(BaseModel):
    role: Literal["assistant", "error"] = Field(..., description="Role (assistant for content, error for issues)")
    content: str = Field(..., description="Content chunk or error message")


# --- Document / RAG Schemas ---

class IndexRequest(BaseModel):
    # Typically handled by Form data in the endpoint (file + metadata)
    # This schema isn't directly used for the /index endpoint request body
    # but represents the conceptual data involved.
    filename: str
    collection_name: Optional[str] = "default"
    metadata: Optional[Dict[str, Any]] = None

class IndexResponse(BaseModel):
    message: str = Field("Document accepted for indexing.", description="Status message")
    filename: str = Field(..., description="Name of the indexed file")
    collection: str = Field(..., description="Collection it was indexed into")
    task_id: Optional[str] = Field(None, description="ID of the background indexing task")
    # s3_key: Optional[str] = Field(None, description="S3 key if uploaded to storage")


class QueryRequest(BaseModel):
    query: str = Field(..., description="The question to ask the documents")
    collection_name: Optional[str] = Field("default", description="Vector store collection to query")
    top_k: int = Field(3, ge=1, le=20, description="Number of relevant documents to retrieve")
    filter: Optional[Dict[str, Any]] = Field(None, description="Metadata filter for vector search (if supported by store)")
    # Example filter: {"source": "myfile.pdf"}

class DocumentMetadata(BaseModel):
    source: str = Field(..., description="Original source identifier (e.g., filename)")
    metadata: Dict[str, Any] = Field({}, description="All metadata associated with the document chunk")
    score: Optional[float] = Field(None, description="Relevance score from vector search (if available)")
    page_content_preview: Optional[str] = Field(None, description="A short preview of the document chunk content")

    # Optional: Helper to create from LangChain doc
    @classmethod
    def from_langchain_doc(cls, doc) -> 'DocumentMetadata':
        score = doc.metadata.pop('score', None) # Extract score if retriever added it
        preview = (doc.page_content[:200] + "...") if len(doc.page_content) > 200 else doc.page_content
        return cls(
            source=doc.metadata.get("source", "Unknown"),
            metadata=doc.metadata,
            score=score,
            page_content_preview=preview
        )


class QueryResponse(BaseModel):
    answer: str = Field(..., description="The answer generated by the LLM based on retrieved documents")
    sources: List[DocumentMetadata] = Field([], description="List of source document chunks used to generate the answer")